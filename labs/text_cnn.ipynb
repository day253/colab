{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNt30dvc3z+voMuPSPwpmqG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/day253/labs/blob/master/labs/text_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "76uJpeSSYVMD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 定义模型\n",
        "我们将定义一个包含两个通道（静态和非静态）的CNN模型。模型结构如下：\n",
        "\n",
        "输入层：句子表示为\n",
        "n\n",
        "×\n",
        "k\n",
        "n×k 的矩阵，其中\n",
        "n\n",
        "n 是句子长度，\n",
        "k\n",
        "k 是词向量维度。\n",
        "\n",
        "卷积层：使用多个卷积核宽度（filter widths）和特征图（feature maps）。\n",
        "\n",
        "池化层：使用时间维度上的最大池化（max-over-time pooling）。\n",
        "\n",
        "全连接层：包含dropout和softmax输出。"
      ],
      "metadata": {
        "id": "GnqtoDnqYx4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_filters, filter_sizes, num_classes, dropout_rate):\n",
        "        super(TextCNN, self).__init__()\n",
        "\n",
        "        # 词嵌入层\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # 静态通道（不更新词嵌入）\n",
        "        self.static_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.static_embedding.weight.requires_grad = False\n",
        "\n",
        "        # 卷积层\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(fs, embedding_dim))\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "\n",
        "        # 全连接层\n",
        "        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 获取词嵌入\n",
        "        embedded = self.embedding(x).unsqueeze(1)  # (batch_size, 1, seq_len, embedding_dim)\n",
        "        static_embedded = self.static_embedding(x).unsqueeze(1)  # (batch_size, 1, seq_len, embedding_dim)\n",
        "\n",
        "        # 合并静态和非静态通道\n",
        "        combined_embedded = torch.cat([embedded, static_embedded], dim=1)  # (batch_size, 2, seq_len, embedding_dim)\n",
        "\n",
        "        # 卷积操作\n",
        "        conv_outputs = [F.relu(conv(combined_embedded)).squeeze(3) for conv in self.convs]  # [(batch_size, num_filters, seq_len - filter_size + 1)]\n",
        "\n",
        "        # 最大池化\n",
        "        pooled_outputs = [F.max_pool1d(conv_output, conv_output.size(2)).squeeze(2) for conv_output in conv_outputs]  # [(batch_size, num_filters)]\n",
        "\n",
        "        # 合并所有池化结果\n",
        "        pooled = torch.cat(pooled_outputs, 1)  # (batch_size, num_filters * len(filter_sizes))\n",
        "\n",
        "        # Dropout\n",
        "        pooled = self.dropout(pooled)\n",
        "\n",
        "        # 全连接层\n",
        "        logits = self.fc(pooled)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "rmHSoPW8YiXA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 初始化模型\n",
        "假设我们有以下参数：\n",
        "\n",
        "vocab_size：词汇表大小\n",
        "\n",
        "embedding_dim：词向量维度\n",
        "\n",
        "num_filters：每个卷积核的特征图数量\n",
        "\n",
        "filter_sizes：卷积核宽度列表\n",
        "\n",
        "num_classes：分类类别数量\n",
        "\n",
        "dropout_rate：dropout率"
      ],
      "metadata": {
        "id": "R2nTdC7RY3lC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "embedding_dim = 300\n",
        "num_filters = 100\n",
        "filter_sizes = [3, 4, 5]\n",
        "num_classes = 2\n",
        "dropout_rate = 0.5\n",
        "\n",
        "model = TextCNN(vocab_size, embedding_dim, num_filters, filter_sizes, num_classes, dropout_rate)"
      ],
      "metadata": {
        "id": "zXuDLGgpYu5V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. 训练模型\n",
        "训练模型的步骤包括：\n",
        "\n",
        "定义损失函数和优化器\n",
        "\n",
        "迭代训练数据\n",
        "\n",
        "计算损失并反向传播"
      ],
      "metadata": {
        "id": "5iB7u7I9Y-zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 假设我们有一些训练数据\n",
        "# train_data: (batch_size, seq_len)\n",
        "# train_labels: (batch_size)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 前向传播\n",
        "    outputs = model(train_data)\n",
        "\n",
        "    # 计算损失\n",
        "    loss = criterion(outputs, train_labels)\n",
        "\n",
        "    # 反向传播和优化\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "gtxu6MtlYymd",
        "outputId": "122f2458-e254-49d3-f27d-061d2a28b05d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'num_epochs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-c6f9c5a5032e>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# train_labels: (batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_epochs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. 测试模型\n",
        "在测试数据上评估模型的性能："
      ],
      "metadata": {
        "id": "xQGeMYZCZGEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(test_data)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    accuracy = (predicted == test_labels).sum().item() / test_labels.size(0)\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "9TNehHiSZD6R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}